<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>Fine tuning - Experimetos IA</title><meta name="description" content="Guía: Fine-Tuning de Llama 3.2 en Mac (MLX) y Preparación para GGUF En esta documentación detallo paso a paso el proceso realizado para&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./fine-tuning.html"><link rel="stylesheet" href="./assets/css/fontawesome-all.min.css?v=85514f933f9e0b82460af63f1a403fa5"><link rel="stylesheet" href="./assets/css/style.css?v=6d92336350d5374cc2b2fb5720b76be5"><noscript><link rel="stylesheet" href="./assets/css/noscript.css?v=efa867a99f5064d6729e4dc2008ad50b"></noscript><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./fine-tuning.html"},"headline":"Fine tuning","datePublished":"2026-02-01T22:55+01:00","dateModified":"2026-02-01T23:13+01:00","description":"Guía: Fine-Tuning de Llama 3.2 en Mac (MLX) y Preparación para GGUF En esta documentación detallo paso a paso el proceso realizado para&hellip;","author":{"@type":"Person","name":"Daniel Terroba Alcala","url":"./authors/daniel-terroba-alcala/"},"publisher":{"@type":"Organization","name":"Daniel Terroba Alcala"}}</script><style>#wrapper > .bg {
               background-image: url(./assets/images/overlay.png), linear-gradient(0deg, rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.1)), url();
           }</style><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="is-preload page-template"><div id="wrapper"><header id="header"><a class="logo" href="./">Experimetos IA</a></header><nav id="nav"><ul class="links"><li><a href="./cuantizacion.html" target="_self">Cuantizacion</a></li><li><a href="./agente-opencode.html" target="_self">Agente</a></li><li class="active"><a href="./fine-tuning.html" target="_self">Fine Tuning</a></li><li><a href="./preentrenamiento.html" target="_self">Preentrenamiento</a></li></ul></nav><main id="main"><article class="post"><header class="major"><h1>Fine tuning</h1><p class="post__inner"></p></header><div class="post__inner post__entry"><h1 style="color: #58a6ff; font-size: 2em; margin-bottom: 10px;">Guía: Fine-Tuning de Llama 3.2 en Mac (MLX) y Preparación para GGUF</h1><p> </p><figure class="post__image"><img loading="lazy" src="./media/posts/3/Gemini_Generated_Image_434c8434c8434c84.png" alt="" width="2816" height="1536" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/Gemini_Generated_Image_434c8434c8434c84-xs.png 300w, ./media/posts/3/responsive/Gemini_Generated_Image_434c8434c8434c84-sm.png 480w, ./media/posts/3/responsive/Gemini_Generated_Image_434c8434c8434c84-md.png 768w, ./media/posts/3/responsive/Gemini_Generated_Image_434c8434c8434c84-lg.png 1024w, ./media/posts/3/responsive/Gemini_Generated_Image_434c8434c8434c84-xl.png 1360w, ./media/posts/3/responsive/Gemini_Generated_Image_434c8434c8434c84-2xl.png 1600w"></figure><p> </p><p>En esta documentación detallo paso a paso el proceso realizado para entrenar un modelo de Inteligencia Artificial (Llama 3.2 1B) directamente en un Mac con Apple Silicon utilizando la librería <strong>MLX</strong>, y cómo solucionar los problemas comunes al prepararlo para su conversión a formato GGUF (para usar en Ollama o Llama.cpp).</p><hr><h2>Paso 1: Configuración del Entorno</h2><p>Lo primero es crear un espacio de trabajo limpio y asegurarnos de que tenemos las librerías necesarias instaladas.</p><div class="command-explanation"><strong>Comandos ejecutados:</strong> Creamos la carpeta del proyecto y entramos en ella. Intentamos instalar las librerías necesarias.</div><pre><code>mkdir entreno-daniel
cd entreno-daniel</code></pre><div class="image-placeholder"> </div><figure class="post__image"><img loading="lazy" src="./media/posts/3/1.jpeg" alt="" width="1920" height="1080" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/1-xs.jpeg 300w, ./media/posts/3/responsive/1-sm.jpeg 480w, ./media/posts/3/responsive/1-md.jpeg 768w, ./media/posts/3/responsive/1-lg.jpeg 1024w, ./media/posts/3/responsive/1-xl.jpeg 1360w, ./media/posts/3/responsive/1-2xl.jpeg 1600w"></figure><div class="image-caption"> </div><h3>Solución al error de PIP</h3><p>En macOS, a veces el comando <code>pip</code> directo da problemas de permisos o no está en el PATH. La forma recomendada y más segura es invocarlo a través de Python:</p><pre><code>python3 -m pip install mlx-lm huggingface_hub</code></pre><div class="image-placeholder"> </div><figure class="post__image"><img loading="lazy" src="./media/posts/3/2.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/2-xs.jpeg 300w, ./media/posts/3/responsive/2-sm.jpeg 480w, ./media/posts/3/responsive/2-md.jpeg 768w, ./media/posts/3/responsive/2-lg.jpeg 1024w, ./media/posts/3/responsive/2-xl.jpeg 1360w, ./media/posts/3/responsive/2-2xl.jpeg 1600w"></figure><div class="image-caption"> </div><hr><h2>Paso 2: Entrenamiento del Modelo (Fine-Tuning)</h2><p>Utilizamos el script <code>lora</code> de MLX. Aquí es donde surgen los errores más comunes relacionados con el formato de los datos.</p><h3>El error del Dataset de Validación</h3><p>Al intentar entrenar, MLX busca por defecto un archivo de validación. Si solo tienes <code>data.jsonl</code> o <code>train.jsonl</code>, el script fallará.</p><div class="alert"><strong>Error común:</strong> <code>ValueError: Validation set not found or empty.</code></div><div class="image-placeholder"><figure class="post__image"><img loading="lazy" src="./media/posts/3/3.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/3-xs.jpeg 300w, ./media/posts/3/responsive/3-sm.jpeg 480w, ./media/posts/3/responsive/3-md.jpeg 768w, ./media/posts/3/responsive/3-lg.jpeg 1024w, ./media/posts/3/responsive/3-xl.jpeg 1360w, ./media/posts/3/responsive/3-2xl.jpeg 1600w"></figure></div><div class="image-caption"> </div><div> </div><div> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/3/4.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/4-xs.jpeg 300w, ./media/posts/3/responsive/4-sm.jpeg 480w, ./media/posts/3/responsive/4-md.jpeg 768w, ./media/posts/3/responsive/4-lg.jpeg 1024w, ./media/posts/3/responsive/4-xl.jpeg 1360w, ./media/posts/3/responsive/4-2xl.jpeg 1600w"></figure></div><h3>Comando de Entrenamiento Correcto</h3><p>Para solucionar esto rápidamente (si no tienes un set de validación separado), copiamos el archivo de entrenamiento. Luego lanzamos el entrenamiento con los siguientes parámetros:</p><ul><li><strong>--model:</strong> El modelo base (Llama-3.2-1B-Instruct-4bit).</li><li><strong>--data:</strong> El directorio actual (.) donde están los .jsonl.</li><li><strong>--iters:</strong> 100 iteraciones (prueba rápida).</li><li><strong>--batch-size:</strong> 1 (para ahorrar memoria).</li><li><strong>--learning-rate:</strong> 1e-5 (tasa de aprendizaje fina).</li></ul><pre><code>cp train.jsonl valid.jsonl

python3 -m mlx_lm.lora \
  --model mlx-community/Llama-3.2-1B-Instruct-4bit \
  --train \
  --data . \
  --iters 100 \
  --batch-size 1 \
  --learning-rate 1e-5 \
  --adapter-path adapters</code></pre><hr><h2>Paso 3: Fusión de Adaptadores (Fuse)</h2><p>Una vez terminado el entrenamiento, tenemos unos "adaptadores" (archivos pequeños con lo aprendido). Para usar el modelo cómodamente, debemos fusionarlos con el modelo base.</p><div class="image-placeholder"> </div><figure class="post__image"><img loading="lazy" src="./media/posts/3/5.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/5-xs.jpeg 300w, ./media/posts/3/responsive/5-sm.jpeg 480w, ./media/posts/3/responsive/5-md.jpeg 768w, ./media/posts/3/responsive/5-lg.jpeg 1024w, ./media/posts/3/responsive/5-xl.jpeg 1360w, ./media/posts/3/responsive/5-2xl.jpeg 1600w"></figure><div class="image-caption"> </div><pre><code>python3 -m mlx_lm.fuse \
  --model mlx-community/Llama-3.2-1B-Instruct-4bit \
  --adapter-path adapters \
  --save-path modelo-daniel-fusionado</code></pre><p class="success">Resultado: Ahora tenemos una carpeta llamada <code>modelo-daniel-fusionado</code> con el modelo completo.</p><hr><h2>Paso 4: Preparación para Llama.cpp (GGUF)</h2><p>Si queremos convertir este modelo a GGUF, necesitamos las herramientas de <code>llama.cpp</code>. Clonamos el repositorio e instalamos sus dependencias.</p><div class="image-placeholder"><figure class="post__image"><img loading="lazy" src="./media/posts/3/6.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/6-xs.jpeg 300w, ./media/posts/3/responsive/6-sm.jpeg 480w, ./media/posts/3/responsive/6-md.jpeg 768w, ./media/posts/3/responsive/6-lg.jpeg 1024w, ./media/posts/3/responsive/6-xl.jpeg 1360w, ./media/posts/3/responsive/6-2xl.jpeg 1600w"></figure></div><pre><code>git clone https://github.com/ggerganov/llama.cpp.git</code></pre><div class="image-placeholder"><figure class="post__image"><img loading="lazy" src="./media/posts/3/7.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/7-xs.jpeg 300w, ./media/posts/3/responsive/7-sm.jpeg 480w, ./media/posts/3/responsive/7-md.jpeg 768w, ./media/posts/3/responsive/7-lg.jpeg 1024w, ./media/posts/3/responsive/7-xl.jpeg 1360w, ./media/posts/3/responsive/7-2xl.jpeg 1600w"></figure></div><p>Instalamos las librerías necesarias para los scripts de conversión de Python:</p><pre><code>python3 -m pip install gguf protobuf sentencepiece torch transformers numpy</code></pre><hr><h2>Paso 5: Cirugía del Modelo (Corrección de Errores)</h2><p>Los modelos cuantizados en MLX (4-bit) a veces dejan "basura" en los archivos de configuración o tensores que son incompatibles con los scripts de conversión de <code>llama.cpp</code>. Tuvimos que realizar dos correcciones manuales mediante Python.</p><h3>Corrección 1: Limpiar <code>config.json</code></h3><p>El archivo de configuración tenía una etiqueta <code>quantization_config</code> sobrante que impedía la conversión. Creamos un script rápido para eliminarla.</p><div class="image-placeholder"><figure class="post__image"><img loading="lazy" src="./media/posts/3/8.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/8-xs.jpeg 300w, ./media/posts/3/responsive/8-sm.jpeg 480w, ./media/posts/3/responsive/8-md.jpeg 768w, ./media/posts/3/responsive/8-lg.jpeg 1024w, ./media/posts/3/responsive/8-xl.jpeg 1360w, ./media/posts/3/responsive/8-2xl.jpeg 1600w"></figure></div><div class="command-explanation"><strong>Script utilizado:</strong></div><pre><code>python3 -c '
import json
import os

file_path = "modelo-daniel-fusionado/config.json"

try:
    with open(file_path, "r") as f:
        data = json.load(f)

    if "quantization_config" in data:
        del data["quantization_config"]
        print("✅ Éxito! Se ha eliminado la etiqueta problemática.")
        
        with open(file_path, "w") as f:
            json.dump(data, f, indent=4)
    else:
        print("ℹ️ No se encontró la etiqueta, el archivo ya estaba limpio.")

except Exception as e:
    print(f"❌ Error: {e}")
'</code></pre><h3>Corrección 2: Limpiar Tensores Fantasma</h3><p>El script de conversión fallaba con el error <code>ValueError: Can not map tensor 'model.embed_tokens.biases'</code>. Este tensor no debería existir en ciertos modelos Llama. Usamos <code>safetensors</code> para borrarlo quirúrgicamente.</p><div class="image-placeholder"> </div><div> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/3/9.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/9-xs.jpeg 300w, ./media/posts/3/responsive/9-sm.jpeg 480w, ./media/posts/3/responsive/9-md.jpeg 768w, ./media/posts/3/responsive/9-lg.jpeg 1024w, ./media/posts/3/responsive/9-xl.jpeg 1360w, ./media/posts/3/responsive/9-2xl.jpeg 1600w"></figure></div><div> </div><div> </div><div class="command-explanation"><strong>Script utilizado (requiere <code>pip install safetensors</code>):</strong></div><pre><code>python3 -c '
from safetensors.torch import load_file, save_file
import os

path = "modelo-daniel-fusionado/model.safetensors"

try:
    print(f"Cargando {path}...")
    tensors = load_file(path)

    bad_key = "model.embed_tokens.biases"

    if bad_key in tensors:
        print(f"⚠️ ¡Encontrado el intruso! ({bad_key})")
        del tensors[bad_key] # Lo borramos
        save_file(tensors, path) # Guardamos
        print("✅ Operación exitosa: Tensor eliminado y archivo guardado.")
    else:
        print("ℹ️ No se encontró el tensor, el archivo ya estaba limpio.")

except Exception as e:
    print(f"❌ Error: {e}")
'</code></pre><hr><h2>Paso 6: Prueba Final del Modelo</h2><p>Antes de llevarlo a producción, verificamos que el modelo ha "aprendido" los nuevos datos. Probamos el chat preguntando por el dato específico del entrenamiento.</p><div class="image-placeholder"> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/3/10.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/10-xs.jpeg 300w, ./media/posts/3/responsive/10-sm.jpeg 480w, ./media/posts/3/responsive/10-md.jpeg 768w, ./media/posts/3/responsive/10-lg.jpeg 1024w, ./media/posts/3/responsive/10-xl.jpeg 1360w, ./media/posts/3/responsive/10-2xl.jpeg 1600w"></figure></div><div> </div><div> </div><div class="image-caption">Imagen 10: ¡Éxito! El modelo reconoce quién es Daniel Terroba.</div><pre><code>python3 -m mlx_lm.chat \
  --model mlx-community/Llama-3.2-1B-Instruct-4bit \
  --adapter-path adapters</code></pre><p><strong>Resultado:</strong> El modelo responde correctamente: <em>"Daniel Terroba es un chico muy particular, al cual le gusta comer plátanos."</em>, confirmando que el Fine-Tuning ha funcionado.</p><hr><p> </p></div></article></main><footer id="copyright"><p>Hecho por Daniel Terroba Alcalá</p></footer></div><script src="./assets/js/jquery.min.js?v=c9771cc3e90e18f5336eedbd0fffb2cf"></script><script src="./assets/js/jquery.scrollex.min.js?v=f89065e3d988006af9791b44561d7c90"></script><script src="./assets/js/jquery.scrolly.min.js?v=1ed5a78bde1476875a40f6b9ff44fc14"></script><script src="./assets/js/browser.min.js?v=c07298dd19048a8a69ad97e754dfe8d0"></script><script src="./assets/js/breakpoints.min.js?v=81a479eb099e3b187613943b085923b8"></script><script src="./assets/js/util.min.js?v=4201a626f8c9b614a663b3a1d7d82615"></script><script src="./assets/js/main.min.js?v=56233c354bd814758be8bff42f7e13a5"></script><script>/*<![CDATA[*/var images=document.querySelectorAll("img[loading]");for(var i=0;i<images.length;i++){if(images[i].complete){images[i].classList.add("is-loaded")}else{images[i].addEventListener("load",function(){this.classList.add("is-loaded")},false)}};/*]]>*/</script></body></html>