<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>Preentrenamiento - Experimetos IA</title><meta name="description" content="Guía Paso a Paso: Creación de un Nano-LLM basado en el Quijote En este tutorial documentaremos el proceso completo para crear, entrenar y&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./preentrenamiento.html"><link rel="stylesheet" href="./assets/css/fontawesome-all.min.css?v=85514f933f9e0b82460af63f1a403fa5"><link rel="stylesheet" href="./assets/css/style.css?v=6d92336350d5374cc2b2fb5720b76be5"><noscript><link rel="stylesheet" href="./assets/css/noscript.css?v=efa867a99f5064d6729e4dc2008ad50b"></noscript><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./preentrenamiento.html"},"headline":"Preentrenamiento","datePublished":"2026-02-01T23:01+01:00","dateModified":"2026-02-01T23:17+01:00","description":"Guía Paso a Paso: Creación de un Nano-LLM basado en el Quijote En este tutorial documentaremos el proceso completo para crear, entrenar y&hellip;","author":{"@type":"Person","name":"Daniel Terroba Alcala","url":"./authors/daniel-terroba-alcala/"},"publisher":{"@type":"Organization","name":"Daniel Terroba Alcala"}}</script><style>#wrapper > .bg {
               background-image: url(./assets/images/overlay.png), linear-gradient(0deg, rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.1)), url();
           }</style><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="is-preload page-template"><div id="wrapper"><header id="header"><a class="logo" href="./">Experimetos IA</a></header><nav id="nav"><ul class="links"><li><a href="./cuantizacion.html" target="_self">Cuantizacion</a></li><li><a href="./agente-opencode.html" target="_self">Agente</a></li><li><a href="./fine-tuning.html" target="_self">Fine Tuning</a></li><li class="active"><a href="./preentrenamiento.html" target="_self">Preentrenamiento</a></li><li><a href="./destilacion.html" target="_self">Destilación</a></li><li><a href="./crear-lm.html" target="_self">Crear un Llm</a></li></ul></nav><main id="main"><article class="post"><header class="major"><h1>Preentrenamiento</h1><p class="post__inner"></p></header><div class="post__inner post__entry"><h1 style="color: #58a6ff; font-size: 2em; margin-bottom: 10px;">Guía Paso a Paso: Creación de un Nano-LLM basado en el Quijote</h1><p> </p><figure class="post__image"><img loading="lazy" src="./media/posts/4/Gemini_Generated_Image_ti1qyzti1qyzti1q.png" alt="" width="2816" height="1536" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/Gemini_Generated_Image_ti1qyzti1qyzti1q-xs.png 300w, ./media/posts/4/responsive/Gemini_Generated_Image_ti1qyzti1qyzti1q-sm.png 480w, ./media/posts/4/responsive/Gemini_Generated_Image_ti1qyzti1qyzti1q-md.png 768w, ./media/posts/4/responsive/Gemini_Generated_Image_ti1qyzti1qyzti1q-lg.png 1024w, ./media/posts/4/responsive/Gemini_Generated_Image_ti1qyzti1qyzti1q-xl.png 1360w, ./media/posts/4/responsive/Gemini_Generated_Image_ti1qyzti1qyzti1q-2xl.png 1600w"></figure><p> </p><p>En este tutorial documentaremos el proceso completo para crear, entrenar y exportar un Pequeño Modelo de Lenguaje (SLM) utilizando el texto completo de Don Quijote de la Mancha. Todo el proceso se ha realizado en un entorno macOS utilizando la aceleración de hardware de Apple (MPS).</p><h2>Paso 1: Configuración del Entorno</h2><p>Lo primero es organizar nuestro espacio de trabajo. Es una buena práctica crear una carpeta dedicada y un entorno virtual de Python para no mezclar librerías con el sistema operativo.</p><div class="image-placeholder"> <figure class="post__image"><img loading="lazy" src="./media/posts/4/1.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/1-xs.jpeg 300w, ./media/posts/4/responsive/1-sm.jpeg 480w, ./media/posts/4/responsive/1-md.jpeg 768w, ./media/posts/4/responsive/1-lg.jpeg 1024w, ./media/posts/4/responsive/1-xl.jpeg 1360w, ./media/posts/4/responsive/1-2xl.jpeg 1600w"></figure></div><h3>Comandos ejecutados:</h3><pre><code>mkdir quijote_definitivo
cd quijote_definitivo
python3 -m venv venv
source venv/bin/activate</code></pre><p><strong>¿Qué hacen estos comandos?</strong></p><ul><li><code>mkdir</code>: Crea el directorio del proyecto.</li><li><code>python3 -m venv venv</code>: Crea un entorno virtual aislado llamado "venv".</li><li><code>source .../activate</code>: Activa el entorno. Verás que aparece <code>(venv)</code> al inicio de la línea de comandos.</li></ul><hr><h2>Paso 2: Instalación de Librerías</h2><p>Necesitamos instalar las herramientas de Inteligencia Artificial: PyTorch (el motor), Transformers (para la arquitectura del modelo) y otras utilidades.</p><div class="image-placeholder"> </div><div> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/2.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/2-xs.jpeg 300w, ./media/posts/4/responsive/2-sm.jpeg 480w, ./media/posts/4/responsive/2-md.jpeg 768w, ./media/posts/4/responsive/2-lg.jpeg 1024w, ./media/posts/4/responsive/2-xl.jpeg 1360w, ./media/posts/4/responsive/2-2xl.jpeg 1600w"></figure></div><h3>Comando ejecutado:</h3><pre><code>pip install torch transformers accelerate sentencepiece gguf numpy</code></pre><div class="note"><strong>Nota:</strong> Al estar en Mac, pip descargará automáticamente las versiones optimizadas para procesadores Apple Silicon (ARM64).</div><hr><h2>Paso 3: Obtención de Datos y Herramientas</h2><p>Descargamos el texto crudo del Quijote desde el Proyecto Gutenberg y clonamos el repositorio de <code>llama.cpp</code>, que usaremos al final para la conversión.</p><div class="image-placeholder"> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/3.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/3-xs.jpeg 300w, ./media/posts/4/responsive/3-sm.jpeg 480w, ./media/posts/4/responsive/3-md.jpeg 768w, ./media/posts/4/responsive/3-lg.jpeg 1024w, ./media/posts/4/responsive/3-xl.jpeg 1360w, ./media/posts/4/responsive/3-2xl.jpeg 1600w"></figure></div><div> </div><h3>Comandos ejecutados:</h3><pre><code>curl -o quijote.txt https://www.gutenberg.org/cache/epub/2000/pg2000.txt
git clone https://github.com/ggerganov/llama.cpp</code></pre><hr><h2>Paso 4: Creación del Tokenizador</h2><p>Los modelos no leen texto, leen números. Necesitamos un "Tokenizador" que convierta palabras en números. Usaremos un script personalizado llamado <code>1_tokenizador.py</code>.</p><div class="image-placeholder"> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/4.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/4-xs.jpeg 300w, ./media/posts/4/responsive/4-sm.jpeg 480w, ./media/posts/4/responsive/4-md.jpeg 768w, ./media/posts/4/responsive/4-lg.jpeg 1024w, ./media/posts/4/responsive/4-xl.jpeg 1360w, ./media/posts/4/responsive/4-2xl.jpeg 1600w"></figure></div><div> </div><h3>El script (1_tokenizador.py):</h3><p>Este script utiliza la librería <code>sentencepiece</code> para entrenar un modelo BPE (Byte Pair Encoding) con un vocabulario de 5,000 palabras.</p><pre><code>import sentencepiece as spm
import os

print("--- 1. Entrenando Tokenizador Nativo (SentencePiece) ---")

spm.SentencePieceTrainer.train(
    input='quijote.txt',
    model_prefix='tokenizer',
    vocab_size=5000,
    model_type='bpe',
    user_defined_symbols=['', '', '<s>', '</s>'],
    character_coverage=0.9995
)

# Limpieza de archivos temporales
if os.path.exists("tokenizer.vocab"):
    os.remove("tokenizer.vocab")

print("¡ÉXITO! Se ha creado 'tokenizer.model'")</code></pre><h3>Ejecución:</h3><p>Ejecutamos el script para generar el archivo <code>tokenizer.model</code>.</p><div class="image-placeholder"> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/5.jpeg" alt="" width="1920" height="1080" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/5-xs.jpeg 300w, ./media/posts/4/responsive/5-sm.jpeg 480w, ./media/posts/4/responsive/5-md.jpeg 768w, ./media/posts/4/responsive/5-lg.jpeg 1024w, ./media/posts/4/responsive/5-xl.jpeg 1360w, ./media/posts/4/responsive/5-2xl.jpeg 1600w"></figure></div><div> </div><pre><code>python3 1_tokenizador.py</code></pre><hr><h2>Paso 5: Entrenamiento del Modelo (El script principal)</h2><p>Ahora definimos la arquitectura del modelo Llama. Hemos creado una versión "Nano" (muy pequeña) para que pueda entrenarse rápido con fines educativos.</p><div class="image-placeholder"> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/6.jpeg" alt="" width="1920" height="1080" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/6-xs.jpeg 300w, ./media/posts/4/responsive/6-sm.jpeg 480w, ./media/posts/4/responsive/6-md.jpeg 768w, ./media/posts/4/responsive/6-lg.jpeg 1024w, ./media/posts/4/responsive/6-xl.jpeg 1360w, ./media/posts/4/responsive/6-2xl.jpeg 1600w"></figure></div><div> </div><h3>El script (2_entrenar.py):</h3><p>Este script configura un modelo <code>LlamaForCausalLM</code> desde cero (sin pre-entrenamiento) y lo alimenta con nuestro texto.</p><div class="note"><strong>Configuración Clave:</strong> Observa que <code>hidden_size=256</code> y <code>num_hidden_layers=6</code>. Es un modelo diminuto comparado con Llama-3 (que tiene miles de millones de parámetros), pero perfecto para aprender.</div><div class="image-placeholder"> </div><div> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/7.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/7-xs.jpeg 300w, ./media/posts/4/responsive/7-sm.jpeg 480w, ./media/posts/4/responsive/7-md.jpeg 768w, ./media/posts/4/responsive/7-lg.jpeg 1024w, ./media/posts/4/responsive/7-xl.jpeg 1360w, ./media/posts/4/responsive/7-2xl.jpeg 1600w"></figure></div><h3>Comando de ejecución:</h3><pre><code>python3 2_entrenar.py</code></pre><p>Al iniciar, verás advertencias sobre <code>MPS</code>. Esto es <strong>bueno</strong>, significa que el script está usando la GPU de tu Mac para acelerar el cálculo.</p><hr><h2>Paso 6: Finalización y Guardado</h2><p>El entrenamiento progresa. Podemos ver cómo el valor de <code>loss</code> (pérdida) baja, lo que significa que el modelo está aprendiendo a predecir la siguiente palabra del estilo de Cervantes.</p><div class="image-placeholder"> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/8.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/8-xs.jpeg 300w, ./media/posts/4/responsive/8-sm.jpeg 480w, ./media/posts/4/responsive/8-md.jpeg 768w, ./media/posts/4/responsive/8-lg.jpeg 1024w, ./media/posts/4/responsive/8-xl.jpeg 1360w, ./media/posts/4/responsive/8-2xl.jpeg 1600w"></figure></div><div> </div><p>Al finalizar, el script guarda el modelo en la carpeta <code>quijote_final</code>.</p><hr><h2>Paso 7: Conversión a GGUF</h2><p>Para usar este modelo en herramientas como LM Studio, Ollama o localmente de forma optimizada, necesitamos convertirlo al formato GGUF.</p><div class="image-placeholder"> </div><div> </div><div><figure class="post__image"><img loading="lazy" src="./media/posts/4/9.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/9-xs.jpeg 300w, ./media/posts/4/responsive/9-sm.jpeg 480w, ./media/posts/4/responsive/9-md.jpeg 768w, ./media/posts/4/responsive/9-lg.jpeg 1024w, ./media/posts/4/responsive/9-xl.jpeg 1360w, ./media/posts/4/responsive/9-2xl.jpeg 1600w"></figure></div><div> </div><h3>Comando ejecutado:</h3><pre><code>python3 llama.cpp/convert_hf_to_gguf.py quijote_final --outfile quijote.gguf</code></pre><p>Este script toma la carpeta donde Hugging Face guardó el modelo y la comprime en un solo archivo binario.</p><div class="image-placeholder"> <figure class="post__image"><img loading="lazy" src="./media/posts/4/10-2.jpeg" alt="" width="1920" height="1080" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/10-2-xs.jpeg 300w, ./media/posts/4/responsive/10-2-sm.jpeg 480w, ./media/posts/4/responsive/10-2-md.jpeg 768w, ./media/posts/4/responsive/10-2-lg.jpeg 1024w, ./media/posts/4/responsive/10-2-xl.jpeg 1360w, ./media/posts/4/responsive/10-2-2xl.jpeg 1600w"></figure></div><p> </p><figure class="post__image"><img loading="lazy" src="./media/posts/4/11.jpeg" alt="" width="706" height="397" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/4/responsive/11-xs.jpeg 300w, ./media/posts/4/responsive/11-sm.jpeg 480w, ./media/posts/4/responsive/11-md.jpeg 768w, ./media/posts/4/responsive/11-lg.jpeg 1024w, ./media/posts/4/responsive/11-xl.jpeg 1360w, ./media/posts/4/responsive/11-2xl.jpeg 1600w"></figure><p> </p><p><strong>¡Resultado final!</strong> El script confirma la creación de <code>quijote.gguf</code>. En la imagen podemos ver la arquitectura final del modelo:</p><ul><li>Context Length: 512 tokens</li><li>Embedding Length: 256</li><li>Head count: 8</li><li>Tamaño del archivo: aprox 13 MB (Un modelo extremadamente ligero).</li></ul><hr><h2>Conclusión y Recomendaciones</h2><p>Has creado con éxito un ciclo de vida completo de un LLM. Aunque este modelo es "tonto" (debido a su tamaño pequeño y pocos datos de entrenamiento), los pasos técnicos son <strong>exactamente los mismos</strong> que se usan para entrenar modelos gigantes como GPT-4 o Llama 3.</p><p><strong>Recomendaciones para siguientes pasos:</strong></p><ol><li><strong>Aumentar el tamaño:</strong> Prueba cambiar <code>hidden_size</code> a 512 o 1024 en el script de entrenamiento para un modelo más capaz.</li><li><strong>Más datos:</strong> Añade más libros de literatura clásica al archivo <code>quijote.txt</code> antes de entrenar.</li><li><strong>Inferencia:</strong> Ahora puedes usar ese archivo <code>.gguf</code> en cualquier programa compatible para "chatear" con tu versión robótica de Cervantes.</li></ol></div></article></main><footer id="copyright"><p>Hecho por Daniel Terroba Alcalá</p></footer></div><script src="./assets/js/jquery.min.js?v=c9771cc3e90e18f5336eedbd0fffb2cf"></script><script src="./assets/js/jquery.scrollex.min.js?v=f89065e3d988006af9791b44561d7c90"></script><script src="./assets/js/jquery.scrolly.min.js?v=1ed5a78bde1476875a40f6b9ff44fc14"></script><script src="./assets/js/browser.min.js?v=c07298dd19048a8a69ad97e754dfe8d0"></script><script src="./assets/js/breakpoints.min.js?v=81a479eb099e3b187613943b085923b8"></script><script src="./assets/js/util.min.js?v=4201a626f8c9b614a663b3a1d7d82615"></script><script src="./assets/js/main.min.js?v=56233c354bd814758be8bff42f7e13a5"></script><script>/*<![CDATA[*/var images=document.querySelectorAll("img[loading]");for(var i=0;i<images.length;i++){if(images[i].complete){images[i].classList.add("is-loaded")}else{images[i].addEventListener("load",function(){this.classList.add("is-loaded")},false)}};/*]]>*/</script></body></html>