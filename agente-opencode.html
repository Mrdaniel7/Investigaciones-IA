<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><title>Agente opencode - Experimetos IA</title><meta name="description" content="Guía Paso a Paso: Configuración de OpenCode en Red y local Esta guía documenta el proceso exacto que hemos seguido en las capturas,&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./agente-opencode.html"><link rel="stylesheet" href="./assets/css/fontawesome-all.min.css?v=85514f933f9e0b82460af63f1a403fa5"><link rel="stylesheet" href="./assets/css/style.css?v=6d92336350d5374cc2b2fb5720b76be5"><noscript><link rel="stylesheet" href="./assets/css/noscript.css?v=efa867a99f5064d6729e4dc2008ad50b"></noscript><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./agente-opencode.html"},"headline":"Agente opencode","datePublished":"2026-02-01T22:46+01:00","dateModified":"2026-02-01T23:11+01:00","description":"Guía Paso a Paso: Configuración de OpenCode en Red y local Esta guía documenta el proceso exacto que hemos seguido en las capturas,&hellip;","author":{"@type":"Person","name":"Daniel Terroba Alcala","url":"./authors/daniel-terroba-alcala/"},"publisher":{"@type":"Organization","name":"Daniel Terroba Alcala"}}</script><style>#wrapper > .bg {
               background-image: url(./assets/images/overlay.png), linear-gradient(0deg, rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.1)), url();
           }</style><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="is-preload page-template"><div id="wrapper"><header id="header"><a class="logo" href="./">Experimetos IA</a></header><nav id="nav"><ul class="links"><li><a href="./cuantizacion.html" target="_self">Cuantizacion</a></li><li class="active"><a href="./agente-opencode.html" target="_self">Agente</a></li><li><a href="./fine-tuning.html" target="_self">Fine Tuning</a></li><li><a href="./preentrenamiento.html" target="_self">Preentrenamiento</a></li><li><a href="./destilacion.html" target="_self">Destilación</a></li><li><a href="./crear-lm.html" target="_self">Crear un Llm</a></li></ul></nav><main id="main"><article class="post"><header class="major"><h1>Agente opencode</h1><p class="post__inner"></p></header><div class="post__inner post__entry"><div class="guide-container"><h1 style="color: #58a6ff; font-size: 2em; margin-bottom: 10px;">Guía Paso a Paso: Configuración de OpenCode en Red y local</h1><p> </p><figure class="post__image"><img loading="lazy" src="./media/posts/2/Gemini_Generated_Image_fs03c5fs03c5fs03.png" alt="" width="2752" height="1536" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/Gemini_Generated_Image_fs03c5fs03c5fs03-xs.png 300w, ./media/posts/2/responsive/Gemini_Generated_Image_fs03c5fs03c5fs03-sm.png 480w, ./media/posts/2/responsive/Gemini_Generated_Image_fs03c5fs03c5fs03-md.png 768w, ./media/posts/2/responsive/Gemini_Generated_Image_fs03c5fs03c5fs03-lg.png 1024w, ./media/posts/2/responsive/Gemini_Generated_Image_fs03c5fs03c5fs03-xl.png 1360w, ./media/posts/2/responsive/Gemini_Generated_Image_fs03c5fs03c5fs03-2xl.png 1600w"></figure><p> </p><p>Esta guía documenta el proceso exacto que hemos seguido en las capturas, desde la conexión SSH al servidor remoto (Mac M4) hasta la verificación del uso de GPU con modelos locales.</p><div style="background-color: #f0f0f0; color: #333; padding: 15px; border-left: 5px solid #007bff; margin-bottom: 20px;"><strong>Prerrequisito (Conexión SSH):</strong><br>Antes de empezar el paso 1, nos hemos conectado al servidor remoto:<br><code style="background-color: #1e1e1e; padding: 2px 5px; border-radius: 3px; color: #ffffff !important; font-family: monospace;">ssh aulaateca26@192.168.7.114</code></div><hr><div class="step" style="margin-bottom: 40px;"><h2>1. Instalación del CLI</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/1.jpeg" alt="" width="1920" height="1080" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/1-xs.jpeg 300w, ./media/posts/2/responsive/1-sm.jpeg 480w, ./media/posts/2/responsive/1-md.jpeg 768w, ./media/posts/2/responsive/1-lg.jpeg 1024w, ./media/posts/2/responsive/1-xl.jpeg 1360w, ./media/posts/2/responsive/1-2xl.jpeg 1600w"></figure><p>Lo primero que vemos es la instalación de la herramienta en el entorno remoto. Estás logueado en la terminal (<code>aulaateca26@Minideaateca262</code>) y ejecutas el script de instalación.</p><p><strong>Comando ejecutado:</strong></p><pre style="background-color: #1e1e1e; padding: 15px; border-radius: 6px; overflow-x: auto; border: 1px solid #444; margin: 10px 0;"><code style="color: #ffffff !important; font-family: 'Courier New', monospace; font-size: 14px; display: block;">curl -fsSL https://opencode.ai/install | bash</code></pre><p><strong>Explicación:</strong> Este comando descarga la última versión de OpenCode (v1.1.34 en la imagen) y la añade al <code>PATH</code> de tu sistema <code>.zshrc</code>, permitiendo que uses el comando globalmente.</p></div><div class="step" style="margin-bottom: 40px;"><h2>2. Pantalla de Bienvenida e Interfaz</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/2.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/2-xs.jpeg 300w, ./media/posts/2/responsive/2-sm.jpeg 480w, ./media/posts/2/responsive/2-md.jpeg 768w, ./media/posts/2/responsive/2-lg.jpeg 1024w, ./media/posts/2/responsive/2-xl.jpeg 1360w, ./media/posts/2/responsive/2-2xl.jpeg 1600w"></figure><p>Una vez instalado, lanzas la aplicación desde la carpeta de tu proyecto.</p><p><strong>Comando:</strong></p><pre style="background-color: #1e1e1e; padding: 15px; border-radius: 6px; overflow-x: auto; border: 1px solid #444; margin: 10px 0;"><code style="color: #ffffff !important; font-family: 'Courier New', monospace; font-size: 14px; display: block;">opencode</code></pre><p><strong>Análisis de la interfaz:</strong> Esta es la TUI (Text User Interface).</p><ul><li><strong>Barra de búsqueda:</strong> "Ask anything..." para empezar a pedir código o chatear.</li><li><strong>Menú inferior:</strong> Muestra los atajos clave como <code>Ctrl+t</code> (variantes), <code>Tab</code> (agentes) y <code>Ctrl+p</code> (comandos).</li></ul></div><div class="step" style="margin-bottom: 40px;"><h2>3. Configuración de Ollama (IP de Red)</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/3.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/3-xs.jpeg 300w, ./media/posts/2/responsive/3-sm.jpeg 480w, ./media/posts/2/responsive/3-md.jpeg 768w, ./media/posts/2/responsive/3-lg.jpeg 1024w, ./media/posts/2/responsive/3-xl.jpeg 1360w, ./media/posts/2/responsive/3-2xl.jpeg 1600w"></figure><p>Aquí configuras el primer proveedor (Ollama) usando el editor <code>nano</code>. El objetivo es apuntar a la IP de la red local, no a localhost.</p><p><strong>Archivo editado:</strong> <code>~/.opencode/config.json</code></p><p><strong>Configuración clave:</strong></p><pre style="background-color: #1e1e1e; padding: 15px; border-radius: 6px; overflow-x: auto; border: 1px solid #444; margin: 10px 0;"><code style="color: #ffffff !important; font-family: 'Courier New', monospace; font-size: 14px; display: block;">"baseURL": "http://192.168.7.114:11434/v1"</code></pre><p><strong>Explicación:</strong> Observa que usamos la IP <code>192.168.7.114</code>. Esto le dice a OpenCode que busque el modelo en esa dirección específica de la red local.</p></div><div class="step" style="margin-bottom: 40px;"><h2>4. Selección del Modelo Ollama</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/4.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/4-xs.jpeg 300w, ./media/posts/2/responsive/4-sm.jpeg 480w, ./media/posts/2/responsive/4-md.jpeg 768w, ./media/posts/2/responsive/4-lg.jpeg 1024w, ./media/posts/2/responsive/4-xl.jpeg 1360w, ./media/posts/2/responsive/4-2xl.jpeg 1600w"></figure><p>Con la configuración guardada, activas el menú de proveedores usando <strong>Ctrl + a</strong>.</p><p><strong>Acción:</strong> Seleccionar <strong>"Ollama (local)"</strong>.</p><p><strong>Modelo elegido:</strong> <code>gpt-oss:20b</code> (resaltado en naranja).</p><p><strong>Resultado:</strong> OpenCode ha leído correctamente tu JSON y te permite elegir el modelo descargado en el servidor.</p></div><div class="step" style="margin-bottom: 40px;"><h2>5. Prueba de Generación de Código</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/5.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/5-xs.jpeg 300w, ./media/posts/2/responsive/5-sm.jpeg 480w, ./media/posts/2/responsive/5-md.jpeg 768w, ./media/posts/2/responsive/5-lg.jpeg 1024w, ./media/posts/2/responsive/5-xl.jpeg 1360w, ./media/posts/2/responsive/5-2xl.jpeg 1600w"></figure><p>Realizamos una prueba técnica pidiendo: <em>"Cálculo de 100 primeros primos en varios lenguajes"</em>.</p><p><strong>Resultados visibles:</strong></p><ul><li>Generación de una función correcta en C++ usando <code>std::vector</code>.</li><li><strong>Tiempo de compilación/generación:</strong> <code>Build gpt-oss:20b . 59.5s</code>. El modelo tardó cerca de un minuto en procesar la solicitud compleja.</li></ul></div><div class="step" style="margin-bottom: 40px;"><h2>6. Configuración de LM Studio</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/6.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/6-xs.jpeg 300w, ./media/posts/2/responsive/6-sm.jpeg 480w, ./media/posts/2/responsive/6-md.jpeg 768w, ./media/posts/2/responsive/6-lg.jpeg 1024w, ./media/posts/2/responsive/6-xl.jpeg 1360w, ./media/posts/2/responsive/6-2xl.jpeg 1600w"></figure><p>Ahora añadimos un segundo proveedor: <strong>LM Studio</strong>. Volvemos a editar el JSON.</p><p><strong>Detalle:</strong> En esta captura se ve la edición de la <code>baseURL</code> para apuntar al puerto <code>1234</code>. Inicialmente se ve un pequeño error en la URL (<code>/v1/responses</code>) que corregiremos más adelante para seguir el estándar de OpenAI.</p></div><div class="step" style="margin-bottom: 40px;"><h2>7. Cambio de Modelo a LM Studio</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/7.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/7-xs.jpeg 300w, ./media/posts/2/responsive/7-sm.jpeg 480w, ./media/posts/2/responsive/7-md.jpeg 768w, ./media/posts/2/responsive/7-lg.jpeg 1024w, ./media/posts/2/responsive/7-xl.jpeg 1360w, ./media/posts/2/responsive/7-2xl.jpeg 1600w"></figure><p>Tras guardar la nueva configuración, el menú de selección ahora muestra ambos proveedores.</p><p><strong>Selección:</strong> Cambiamos a <strong>"LM Studio (local)"</strong> y elegimos el modelo <strong>"Llama 3 (Local)"</strong>.</p><p><strong>Flexibilidad:</strong> Esto demuestra que puedes alternar entre Ollama y LM Studio en la misma sesión sin salir del programa.</p></div><div class="step" style="margin-bottom: 40px;"><h2>8. Refinamiento Final del JSON</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/8.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/8-xs.jpeg 300w, ./media/posts/2/responsive/8-sm.jpeg 480w, ./media/posts/2/responsive/8-md.jpeg 768w, ./media/posts/2/responsive/8-lg.jpeg 1024w, ./media/posts/2/responsive/8-xl.jpeg 1360w, ./media/posts/2/responsive/8-2xl.jpeg 1600w"></figure><p>Vemos la configuración final y correcta para LM Studio dentro del archivo <code>config.json</code>.</p><p><strong>Configuración Estándar Correcta:</strong></p><pre style="background-color: #1e1e1e; padding: 15px; border-radius: 6px; overflow-x: auto; border: 1px solid #444; margin: 10px 0;"><code style="color: #ffffff !important; font-family: 'Courier New', monospace; font-size: 14px; display: block;">"lmstudio": {
  "npm": "@ai-sdk/openai-compatible",
  "name": "LM Studio Local",
  "options": {
    "baseURL": "http://127.0.0.1:1234/v1"
  },
  "models": {
    "meta-llama-3-8b-instruct": {
      "name": "Llama 3 (Local)"
    }
  }
}</code></pre><p>Nota que la URL base ahora termina correctamente en <code>/v1</code> y usamos <code>127.0.0.1</code> porque LM Studio corre en la misma máquina física (el servidor M4).</p></div><div class="step" style="margin-bottom: 40px;"><h2>9. Verificación de Hardware y Chat en Español</h2><figure class="post__image"><img loading="lazy" src="./media/posts/2/9.jpeg" alt="" width="1599" height="899" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/2/responsive/9-xs.jpeg 300w, ./media/posts/2/responsive/9-sm.jpeg 480w, ./media/posts/2/responsive/9-md.jpeg 768w, ./media/posts/2/responsive/9-lg.jpeg 1024w, ./media/posts/2/responsive/9-xl.jpeg 1360w, ./media/posts/2/responsive/9-2xl.jpeg 1600w"></figure><p>La prueba definitiva de funcionamiento.</p><p><strong>Izquierda (Monitor de Actividad):</strong></p><ul><li>La GPU <strong>"Apple M4 (Integrada)"</strong> muestra una carga masiva (barras llenas).</li><li>Esto confirma que la inferencia es local y acelerada por hardware.</li></ul><p><strong>Derecha (Chat OpenCode):</strong></p><ul><li>Interacción fluida en español: <em>"Hola, ¿en qué puedo ayudarte?"</em></li><li>Tiempos de respuesta rápidos (30-36s para respuestas completas).</li></ul><p><strong>Conclusión:</strong> El sistema está totalmente operativo, ejecutando modelos de IA privados en un servidor remoto y controlados vía terminal.</p></div></div></div></article></main><footer id="copyright"><p>Hecho por Daniel Terroba Alcalá</p></footer></div><script src="./assets/js/jquery.min.js?v=c9771cc3e90e18f5336eedbd0fffb2cf"></script><script src="./assets/js/jquery.scrollex.min.js?v=f89065e3d988006af9791b44561d7c90"></script><script src="./assets/js/jquery.scrolly.min.js?v=1ed5a78bde1476875a40f6b9ff44fc14"></script><script src="./assets/js/browser.min.js?v=c07298dd19048a8a69ad97e754dfe8d0"></script><script src="./assets/js/breakpoints.min.js?v=81a479eb099e3b187613943b085923b8"></script><script src="./assets/js/util.min.js?v=4201a626f8c9b614a663b3a1d7d82615"></script><script src="./assets/js/main.min.js?v=56233c354bd814758be8bff42f7e13a5"></script><script>/*<![CDATA[*/var images=document.querySelectorAll("img[loading]");for(var i=0;i<images.length;i++){if(images[i].complete){images[i].classList.add("is-loaded")}else{images[i].addEventListener("load",function(){this.classList.add("is-loaded")},false)}};/*]]>*/</script></body></html>